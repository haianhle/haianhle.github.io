<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>AR::VGG</title>
<!--
-->
    <!-- load stylesheets -->
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open+Sans:300,400">
    <link rel="stylesheet" href="../css/bootstrap.min.css">
    <link rel="stylesheet" href="../css/magnific-popup.css">
    <link rel="stylesheet" href="../css/ar-style.css">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
          <![endif]-->
</head>

<body>
    <div class="container-fluid">
        <section id="welcome" class="tm-content-box tm-banner margin-b-10">
            <div class="tm-banner-inner">
                <h1 class="tm-banner-title"><a href="../index.html" style="text-decoration: None; color:#0d3298"> Anh H. Reynolds</a></h1>
            </div>                    
        </section>
        <!--
        <p align='center' style="font-size:200%;color:#0d3298">Data Science</p>
        -->


        <div class="tm-body">
            <div class="tm-sidebar">
                <nav class="tm-main-nav">
                    <ul class="tm-main-nav-ul">
                        <li class="tm-nav-item"><a href="../datascience.html" class="tm-nav-item-link tm-button">
                            <i class="fa tm-nav-fa"></i>Data Science</a>
                        </li>
                        <li class="tm-nav-item"><a href="../404.html" class="tm-nav-item-link tm-button">
                            <i class="fa tm-nav-fa"></i>Quantum Chemistry</a>
                        </li>
                        <li class="tm-nav-item"><a href="../index.html" class="tm-nav-item-link tm-button">
                            <i class="fa tm-nav-fa"></i>About me</a>
                        </li>
                        <li class="tm-nav-item"><a href="../cv.html" class="tm-nav-item-link tm-button">
                            <i class="fa tm-nav-fa"></i>Curriculum Vitae</a>
                        </li>
                        <li class="tm-nav-item"><a href="../others.html" class="tm-nav-item-link tm-button">
                            <i class="fa tm-nav-fa"></i>Other interests</a>
                        </li>
                    </ul>
                </nav>

            </div>

            <div class="tm-main-content">
                <div class="tm-content-box tm-content-box-home">                        
                    <h2>Large-scale image recognition: VGG</h2>
                    <p align="justify">
                    Source: <a href=https://arxiv.org/abs/1409.1556>original paper</a>.
                    by Karen Simonyan and Andrew Zisserman (2015)
                    </br>
                    </br>
                    VGG stands for Visual Geometry Group, a research group in the 
                    Department of Engineering Science at the University
                    of Oxford, and refers to the deep convolutional network (ConvNet) models either with 
                    16 layers (VGG-16) or 19 layers (VGG-19). These models were shown to provide 
                    excellent accuracy on <a href=http://www.image-net.org/challenges/LSVRC/>ILSVRC</a>
                    (ImageNet Large-Scale Visual Recognition Challenge) 
                    classification and localization tasks.
                    The network consists of a stack of convolutional layers followed by three fully connected (FC)
                    layers with a final 1000-way softmax.
                    Compared to AlexNet in the
                    <a href="alexnet.html">previous post</a>, this network is deeper but
                    has a simpler architechture as illustrated below. 
                    The same structure is used for all the
                    convolutional layers with filters of size \(3\times 3\), same convolution
                    (padding such that output and input sizes or resolutions are the same), and
                    a stride of 1. Similarly, for all pooling layers, max pooling was used with
                    \(2\times 2\) window and a stride of 2. 
                    As the network gets deeper, the height and width go down by a factor of 2 while
                    the number of filters goes up by a factor of 2.
                    The simplicity and uniformity in construction are among the nice
                    features of the network.
                    </p>
                    <center><figure>
                    <img src="vgg.png" width="100%" class="center"></img>
                    <figcaption><font size="3">
                    Architechture of the VGG-16 ConvNet. All the convolutional
                    layers use same convolution \(3\times 3\) filters with stride \(s=1\). All pooling
                    layers use \(2\times 2\) max pooling with stride 2 \((f=2, s=2)\).
                    </font></figcaption>
                    </figure></center>
                    </br>
                    <p align="justify">
                    The activation shapes and sizes as well as the number of parameters for each layer
                    are tabulated below. The total number of parameters is calculated to be 138,348,355.
                    This is considered a very large net work even for current standard.
                    </p>
                    <center><img src="vgg-parameters.png" width="80%" class="center"></img></center> 
                    </br>
                    <p align="justify">
                    It appears to be standard practice as for this model the input images were also 
                    \(224\times 224\times 3\), which was obtained by randomly cropping the rescaled
                    training images. Data augmentation was done as described by
                    <a href=http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ>
                    Krizhevsky et al.</a> in the AlexNet paper.
                    Also similar to the AlexNet paper, the authors normalized
                    the pixel values using the mean RGB value computed on the training set.
                    ReLU activation function was used to add non-linearity.
                    Note that according to the authors, Local Response Normalization, a technique used
                    by Alex Krizhevsky et al. in the AlexNet paper does not result in 
                    performance improvement on the ILSVRC dataset. This is consistent with what Andrew Ng
                    mentioned in his course on CNNs.
                    </p>
                    <p align="justify">
                    Another interesting point from the paper is how the use of a stack of conv layers
                    with small size filters (without pooling in between) is more advantageous to
                    one conv layer with large size filters. In other words, the authors are in favor
                    of deeper networks with small convolution filters. 
                    Supposedly, a stack of two \(3\times 3\) conv
                    layers is effectively equivalent to a single \(5\times 5\) conv layer, and a stack of
                    three \(3\times 3\) conv layers is effectively equivalent to a single \(7\times 7\)
                    conv layer. However, by having a stack of smaller conv layers, first, non-linearity
                    is added more often (after every layer), and second, the number of parameters
                    is much smaller resulting in a regularization effect.
                    </p>
                    <p align="justify">
                    Gradient descent with momentum was also used in training this model with a typical
                    value for the momentum parameter 0.9, weight decay 0.0005. Dropout regularization was used for
                    the first two fully connected layers with dropout ratio set to 0.5. The learning rate
                    was initialized to 0.01, and subsequently decreased by a factor of 10 when the validation
                    set accuracy stopped improving, which is the same procedure used in the AlexNet paper.
                    The network was trained over 370K iterations (74 epochs).
                    The mini-batch size was 256.
                    </p>
                    <p align="justify">
                    Detailed results on the ILSVRC datasets can be found on the paper. The network showed
                    very impressive performance, outperforming many models including  GoogLeNet.
                    </p>
                </div>
            </div>
        </div>

        <footer class="tm-footer">
            <p class="text-xs-center">Copyright &copy; 2019 Anh H. Reynolds</p>
        </footer>

    </div>

    <!-- load JS files -->
    
    <script src="../js/jquery-1.11.3.min.js"></script>
    <script src="https://www.atlasestateagents.co.uk/javascript/tether.min.js"></script> 
    <script src="../js/jquery.magnific-popup.min.js"></script>
    <script src="../js/jquery.singlePageNav.min.js"></script>
    <script src="../js/vendor/modernizr-3.7.1.min.js"></script>
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="js/vendor/jquery-3.3.1.min.js"><\/script>')</script>
    <script src="../js/plugins.js"></script>
    <script src="../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
    <!-- Templatemo scripts -->
    <script>  

    function setNavbar() {
        if ($(document).scrollTop() > 160) {
            $('.tm-sidebar').addClass('sticky');
        } else {
            $('.tm-sidebar').removeClass('sticky');
        }
    }                   

    $(document).ready(function(){
        
        // Single page nav
        $('.tm-main-nav').singlePageNav({
            'currentClass' : "active",
            offset : 20
        });

        // Detect window scroll and change navbar
        setNavbar();
        
        $(window).scroll(function() {
          setNavbar();
        });

        // Magnific pop up
        $('.tm-gallery').magnificPopup({
          delegate: 'a', // child items selector, by clicking on it popup will open
          type: 'image',
          gallery: {enabled:true}
          // other options
        });
    });

    </script>             

</body>
</html>
