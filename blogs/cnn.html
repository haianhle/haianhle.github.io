<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>AR::CNNs</title>
<!--
-->
    <!-- load stylesheets -->
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open+Sans:300,400">
    <link rel="stylesheet" href="../css/bootstrap.min.css">
    <link rel="stylesheet" href="../css/magnific-popup.css">
    <link rel="stylesheet" href="../css/ar-style.css">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
          <![endif]-->
</head>

<body>
    <div class="container-fluid">
        <section id="welcome" class="tm-content-box tm-banner margin-b-10">
            <div class="tm-banner-inner">
                <h1 class="tm-banner-title"><a href="../index.html" style="text-decoration: None; color:#0d3298"> Anh H. Reynolds</a></h1>
            </div>                    
        </section>
        <!--
        <p align='center' style="font-size:200%;color:#0d3298">Data Science</p>
        -->


        <div class="tm-body">
            <div class="tm-sidebar">
                <nav class="tm-main-nav">
                    <ul class="tm-main-nav-ul">
                        <li class="tm-nav-item"><a href="../datascience.html" class="tm-nav-item-link tm-button">
                            <i class="fa tm-nav-fa"></i>Data Science</a>
                        </li>
                        <li class="tm-nav-item"><a href="../404.html" class="tm-nav-item-link tm-button">
                            <i class="fa tm-nav-fa"></i>Quantum Chemistry</a>
                        </li>
                        <li class="tm-nav-item"><a href="../cv.html" class="tm-nav-item-link tm-button">
                            <i class="fa tm-nav-fa"></i>Curriculum Vitae</a>
                        </li>
                        <li class="tm-nav-item"><a href="../others.html" class="tm-nav-item-link tm-button">
                            <i class="fa tm-nav-fa"></i>Other interests</a>
                        </li>
                    </ul>
                </nav>

            </div>

            <div class="tm-main-content tm-box-pad">
                <div class="tm-content-box tm-content-box-home">                        
                    <h2>Convolutional Neural Networks (CNNs)</h2>
                    </br>
                    </br>
                    <p align="justify">
                    The motivation arises from the fact that a fully connected network grows quickly
                    with the size of an image, which consequently requires an enormous dataset 
                    to avoid overfitting (besides the prohibitive computational cost).
                    Convolutional neural networks, as the name implies, has to do with the convolution 
                    between a kernel (or a filter) and an image in each convolutional layer. 
                    A filter refers to a small matrix, and the
                    convolution operator (denoted as \(*\), sometimes called cross-correlation) gives rise 
                    to a new image where each element is a weighted combination of the entries of a
                    region or patch of the original image, with weights given by the filter. 
                    A convolutional neural network or ConvNet typically consists of some convolutional layers,
                    some pooling layers, and some fully connected layers.
                    </p>
                    </br>
                    <h4>Edge detection</h4>
                    Edge detection is an example of the usefulness of convolution in
                    identifying the regions in an image where there is a sharp change in colors or intesntiy.
                    <p align="justify">
                    </p>
                    <center><img src="edge-detection-vertical.png" width="100%"></img></center>
                    <p align="center">An example of vertical edge detection</p>
                    </br>
                    <p align="justify">
                    For example, in the figure above, applying the convolution operator between
                    the \(3\times 3\) filter (center) and the blue region of the original image (left) gives rise to
                    the element in the new image in blue box (right), whose value can be calculated as 
                    the sum of the resulting element-wise product between the 2 matrices:
                    $$ 0 = 10\times 1 + 10\times 0 + 10\times (-1) + 10\times 1 + 10\times 0 + 10\times (-1)\\
                         + 10\times 1 + 10\times 0 + 10\times (-1)$$
                    </br>
                    This is an example of a vertical edge detector because it can detect the sharp edge in
                    the middle of the original image, shown by the brighter region in the center of the
                    resulting image.
                    </p>
                    </br>
                    <center><img src="edge-detection-horizontal.png" width="100%"></img></center>
                    <p align="center">An example of horizontal edge detection</p>
                    </br>
                    <p align="justify">
                    Another example is shown above with a horizontal edge detector with a slightly more complicated
                    image. The non-zero band in the middle reveals the horizontal edge in the center of the original
                    image but also discovers the change from bright to dark (left side) and dark to bright (right side).
                    </p>
                    </br>
                    <p align="justify">
                    Examples of other commonly used filters are shown below. However, instead of hand-picking
                    which filter to use for each task, the parameters associated with the filter from its size
                    to the values of its matrix elements can be trained by a neural network.
                    </p>
                    <center><img src="filters.png" width="60%" class="center"></img></center>
                    </br>
                    <h4>Padding</h4>
                    <p align="justify">
                    If the original image is of size \(n\times n\) and the filter is of size \(f\times f\), the
                    size of the resulting image is \( (n–f+1)\times (n–f+1)\). As a result, the input image size is
                    going to shrink after each convolutional layer.
                    In addition, because the edge of the 
                    original image is not used in the convolution operation as often as the central pixels,
                    some valuable information is potentially lost in the process.
                    </br>
                    The solution to these problems is through padding the border of the original image with
                    \(p\) extra layer(s) of zeros in every direction. The dimensions of the input and output 
                    images become \((n+2p)\times (n+2p)\) and \((n+2p–f+1)\times(n+2p–f+1)\) respectively.
                    When \(p=0\), that is, no padding, this is called "valid" convolution.
                    When \(p = (f–1)/2\) so that the sizes of the input and output images are the same, this is called
                    "same" convolution.
                    </p>
                    <center><img src="padding.png" width="100%" class="center"></img></center>
                    <p align="center">An example of same convolution with padding \(p=1\)</p>
                    <h4>Convolutional layer</h4>
                    <ul>
                    <li><p align="justify">the convolution operator can be applied to the original image with a stride
                    \(s\) other than 1, resulting in fewer operations and output image of smaller size.
                    $$\left(\frac{n+2p–f}{s}+1\right)\times \left(\frac{n+2p–f}{s}+1\right)$$
                    </p></li>
                    <li><p align="justify">the numbers of channels must be the same for the input image and the filter.
                    See example below for an image with 3 channels (RGB) of dimensions \(6\times 6\times 3\).
                    The filter has dimensions \(3\times 3\times 3\), where the last \(3\) is the number
                    of channels. The resulting image has dimension \(4\times 4\), 
                    assuming \(s=1\) and \(p=0\).</p></li>
                    <center><img src="3D-conv.png" width="70%" class="center"></img></center>
                    <li><p align="justify">multiple filters can be applied to each image. Given an input image of dimensions
                    \(n\times n\times n_C\) and \(n_C'\) filters of dimensions \(f\times f\times n_C\)
                    with stride \(s\) and padding \(p\),
                    the dimensions of the resulting output is
                    $$\left(\frac{n+2p–f}{s}+1\right)\times \left(\frac{n+2p–f}{s}+1\right)\times n_C'$$
                    </p></li>
                    <li><p align="justify">the number of parameters to be learned in each convolutional layer is
                    \((f\times f\times n_C'+1)\times n_C\), which is independent of the size of the input image.
                    This helps solve the two problems mentioned at the very beginning of this post.
                    Note that \(f, n_C, n_C', p, s\) are all hyperparameters and are not trained in the network,
                    that is, must be tuned outside of the network.
                    </p></li>
                    </ul>
                    </br>
                    <h5>Notation</h5>
                    <p align="justify">
                    For layer \(l\), the filter size, padding, stride are denoted
                    as \(f^{[l]}, p^{[l]}, s^{[l]}\) respectively.
                    </br>
                    The dimensions of the input image:
                    \(n_H^{[l–1]}\times n_W^{[l–1]}\times n_C^{[l–1]}\)
                    </br>
                    Dimensions of the output image:
                    \(n_H^{[l]}\times n_W^{[l]}\times n_C^{[l]}\)
                    </br>
                    where
                    $$n_H^{[l]} = \frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1$$
                    $$n_W^{[l]} = \frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1$$
                    and \(n_C^{[l]}\) is the number of filters used in layer \(l\).
                    </br>
                    Size of each filter: \(f^{[l]}\times f^{[l]}\times n_C^{[l–1]}\)
                    </br>
                    (because the number of channels of the filter must match that of the input image)
                    </br>
                    Each weight tensor has the same size as a the filter. Since there are
                    \(n_C\) filters, dimensions of the weight tensor \(w^{[l]}\) in layer \(l\) is
                    \(f^{[l]}\times f^{[l]}\times n_C^{[l–1]}\times n_C^{[l]}\),
                    and the bias vector has dimensions \(1\times 1\times 1\times n_C\).
                    The activation function \(a^{[l]}\) applies nonlinearity on all
                    the output images, and as a result has dimensions
                    \(m\times n_H^{[l]}\times n_W^{[l]}\times n_C^{[l]}\) where
                    \(m\) is the number of images, that is, the number of training examples.
                    </p>
                    </br>
                    <h4>Pooling layer</h4>
                    <p align="justify">
                    The motivation behind pooling layers is to reduce the size of the representation,
                    keeping only the more important features. Pooling layers have been found to work very
                    well, but the underlying reasons for that are not fully understood. 
                    The most common type of pooling layers used is Max Pooling; the less common 
                    Average Pooling is sometimes seen in very deep neural networks.
                    </br>
                    </p>
                    <center><img src="max-pooling.png" width="50%" class="center"></img></center>
                    </br>
                    <p align="justify">
                    Pooling is applied very similarly to the convolution operator by sliding 
                    a small matrix of size \(f\times f\) across the input image with stride \(s\).
                    There is almost always no padding, \(p=0\). The Max Pooling operation involves
                    taking the max value, while the Average Pooling, the average. An example is shown
                    for \(f=2\) and \(s=2\) applied on a \(4\times 4\) matrix \((n=4)\). Unlike convolution,
                    pooling is applied to each channel individually, as a result, the number of channels is
                    preserved. The dimensions of the output given input dimensions 
                    \(n_H\times n_W\times n_C\) are 
                    $$\left(\frac{n_H-f}{s}+1\right)\times \left(\frac{n_W-f}{s}+1\right)\times n_C$$
                    Since there is no parameter to be learned in a pooling layer, it is often not
                    counted in the number of total layers in a ConvNet.
                    </p>
                    </br>
                    <h4>Example of a ConvNet</h4>
                    <p align="justify">
                    This simple network that is very similar to <a href="http://yann.lecun.com/exdb/lenet/">
                    LeNet-5</a> is an example given by Andrew Ng
                    in Course 4 of the <a href="https://www.coursera.org/specializations/deep-learning#courses">
                    Deep Learning Specialization</a> on Coursera. 
                    A ConvNet usually consists
                    of a few convolutional layers (each is often followed by a max pooling layer), and
                    a few fully connected layers.
                    </p>
                    </br>
                    <center><img src="lenet-5.png" width="100%" class="center"></img></center> 
                    </br>
                    <p align="justify">
                    Generally, we can see that \(n_H, n_W\downarrow\) while \(n_C\uparrow\).
                    The size of the activations should gradually decrease.
                    Most of the parameters to be learned in a ConvNet are in the last few fully connected
                    layers as seen in the table below.
                    </p>
                    </br>
                    <center><img src="lenet-5-parameters.png" width="80%" class="center"></img></center> 
                    </br>
                    <p align="justify">
                    The original LeNet-5 was on grey scale images, so the dimensions of the original
                    input image is \(32\times 32\times 1\).
                    In addition, \(tanh\) and \(sigmoid\) activation functions were preferred to ReLU
                    at the time the paper was published.
                    </p>
                    </br>
                    <h4>Why ConvNet?</h4>
                    2 main advantages of using a ConvNet in computer vision applications are
                    <ul>
                    <li><p align="justify">parameter sharing: a feature detector such as vertical
                    edge detector that is useful in one part of the image is probably be useful in
                    another part of the image. This is often true for both low level features
                    and high level features.</p></li>
                    <li><p align="justify">sparsity of connection: the output unit depends only on a small
                    number of input units. As a result, a ConvNet is more robust and less prone
                    to overfitting. </p></li>
                    </ul>
                </div>
            </div>
        </div>

        <footer class="tm-footer">
            <p class="text-xs-center">Copyright &copy; 2019 Anh H. Reynolds</p>
        </footer>

    </div>

    <!-- load JS files -->
    
    <script src="../js/jquery-1.11.3.min.js"></script>
    <script src="https://www.atlasestateagents.co.uk/javascript/tether.min.js"></script> 
    <script src="../js/jquery.magnific-popup.min.js"></script>
    <script src="../js/jquery.singlePageNav.min.js"></script>
    <script src="../js/vendor/modernizr-3.7.1.min.js"></script>
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="js/vendor/jquery-3.3.1.min.js"><\/script>')</script>
    <script src="../js/plugins.js"></script>
    <script src="../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
    <!-- Templatemo scripts -->
    <script>  

    function setNavbar() {
        if ($(document).scrollTop() > 160) {
            $('.tm-sidebar').addClass('sticky');
        } else {
            $('.tm-sidebar').removeClass('sticky');
        }
    }                   

    $(document).ready(function(){
        
        // Single page nav
        $('.tm-main-nav').singlePageNav({
            'currentClass' : "active",
            offset : 20
        });

        // Detect window scroll and change navbar
        setNavbar();
        
        $(window).scroll(function() {
          setNavbar();
        });

        // Magnific pop up
        $('.tm-gallery').magnificPopup({
          delegate: 'a', // child items selector, by clicking on it popup will open
          type: 'image',
          gallery: {enabled:true}
          // other options
        });
    });

    </script>             

</body>
</html>
